{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xanderread/Recommender_Cswrk/blob/main/Recommender_Systems_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installs\n",
        "!pip install scikit-surprise\n",
        "!pip install torch\n",
        "!pip install torch-geometric\n",
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "print(\"installs done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM2lEYjYMWdr",
        "outputId": "5782049c-eb98-48ba-c9c5-a3b300535a1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.10/dist-packages (1.1.3)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.11.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: torch_geometric 2.4.0\n",
            "Uninstalling torch_geometric-2.4.0:\n",
            "  Successfully uninstalled torch_geometric-2.4.0\n",
            "\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_scatter-2.1.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu118\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_sparse-0.6.18%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.23.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu118\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_cluster-1.6.3%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.23.5)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt21cu118\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-o464fx9x\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-o464fx9x\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 9068ad70e7c46c807d39747d2a70cba7836ea82d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.1.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (5.9.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric==2.4.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (3.2.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.4.0-py3-none-any.whl size=1074618 sha256=8bfcbce60b0651cac70235596c21ca7e2bcf3ed5d90a939f8a45fc3cff81f4f1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5kfpo0mi/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iyPHKYEnegPV"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from urllib.request import urlretrieve\n",
        "import zipfile\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from surprise import SVD, accuracy, Reader, Dataset\n",
        "from surprise.model_selection import PredefinedKFold,KFold\n",
        "import re\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, metrics, preprocessing\n",
        "import copy\n",
        "from torch_geometric.utils import degree\n",
        "import torch\n",
        "from torch import nn, optim, Tensor\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from torch_geometric.utils import structured_negative_sampling\n",
        "from torch_geometric.data import download_url, extract_zip\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.typing import Adj\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WVlwFIUUegPY"
      },
      "outputs": [],
      "source": [
        "# get dataset / unzip\n",
        "urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\", \"movielens.zip\")\n",
        "zip_ref = zipfile.ZipFile('movielens.zip', \"r\")\n",
        "zip_ref.extractall()\n",
        "\n",
        "\n",
        "# only using age of the user in methods\n",
        "users_cols = ['user_id', 'age']\n",
        "users = pd.read_csv('ml-100k/u.user', sep='|', names=users_cols, usecols=['user_id', 'age'], encoding='latin-1')\n",
        "\n",
        "# load ratings - without unix timestamp\n",
        "ratings_cols = ['user_id', 'movie_id', 'rating']\n",
        "# Load the data specifying the columns\n",
        "ratings = pd.read_csv(\n",
        "    'ml-100k/u.data', sep='\\t', names=ratings_cols, usecols=['user_id', 'movie_id', 'rating'], encoding='latin-1')\n",
        "\n",
        "# load the movies data into a pandas frame\n",
        "# this data is not used in the methods but is used to visualise the data when explaining the data in the demonstration video\n",
        "genre_cols = [\n",
        "    \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
        "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
        "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
        "]\n",
        "movies_cols = [\n",
        "    'movie_id', 'title', 'release_date', \"video_release_date\", \"imdb_url\"\n",
        "] + genre_cols\n",
        "movies = pd.read_csv(\n",
        "    'ml-100k/u.item', sep='|', names=movies_cols, encoding='latin-1')\n",
        "\n",
        "\n",
        "# shift the ids of elements to be 0 (they start at 1 in the dataset)\n",
        "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: str(x-1))\n",
        "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: str(x-1))\n",
        "movies[\"year\"] = movies['release_date'].apply(lambda x: str(x).split('-')[-1])\n",
        "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: str(x-1))\n",
        "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: str(x-1))\n",
        "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B-3AVLDpegPY"
      },
      "outputs": [],
      "source": [
        "# get the age ratings of the movies using an API, and assign a minium age to each movie\n",
        "# users that are not old enough to watch a movie should not be recommended that movie\n",
        "# however this filtering will take place AFTER the model is trained and is not used as a feature for either implementation\n",
        "\n",
        "age_rating_map = {\n",
        "    'G': 0,\n",
        "    'PG': 10,\n",
        "    'PG-13': 13,\n",
        "    'R': 17,\n",
        "    'NC-17': 18\n",
        "}\n",
        "\n",
        "\n",
        "api_key = \"25a58189\"\n",
        "base_url = f\"http://www.omdbapi.com/?apikey={api_key}&t=\"\n",
        "\n",
        "\n",
        "movies['age_rating'] = pd.NA\n",
        "\n",
        "\n",
        "# iterate over the movies fetching the rating from the api and adding it to the df\n",
        "for index, row in movies.iterrows():\n",
        "    movie_title = row['title'].replace(\" \", \"+\")\n",
        "    # making the API call\n",
        "    response = requests.get(f\"{base_url}{movie_title}\")\n",
        "\n",
        "    if response.status_code == 200 and response.json().get('Response') == 'True':\n",
        "        age_rating = response.json().get('Rated', pd.NA)\n",
        "        # map the age rating to a numerical value\n",
        "        numerical_rating = age_rating_map.get(age_rating, pd.NA)\n",
        "    else:\n",
        "        numerical_rating = pd.NA\n",
        "\n",
        "    # update the DataFrame with the numerical age rating\n",
        "    movies.at[index, 'age_rating'] = numerical_rating\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# the movies ids / user ids are kinda wild - they range from 0 to a very large number which is more than the no of unique users and movies\n",
        "# make so that the no of user ids = the number the user ids go up to\n",
        "# and same for movie ids\n",
        "lbl_user = preprocessing.LabelEncoder()\n",
        "lbl_movie = preprocessing.LabelEncoder()\n",
        "\n",
        "ratings.user_id = lbl_user.fit_transform(ratings.user_id.values)\n",
        "ratings.movie_id = lbl_movie.fit_transform(ratings.movie_id.values)\n",
        "\n",
        "lbl_user = preprocessing.LabelEncoder()\n",
        "lbl_movie = preprocessing.LabelEncoder()\n",
        "\n",
        "ratings.user_id = lbl_user.fit_transform(ratings.user_id.values)\n",
        "ratings.movie_id = lbl_movie.fit_transform(ratings.movie_id.values)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nX7FjXwSD6V6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# he talks about this COO format - where graphs are represented in this way - you will need to start watching the youtube videos on this\n",
        "# load edges between users and movies\n",
        "def load_edge_csv(df,\n",
        "                  src_index_col,\n",
        "                  dst_index_col,\n",
        "                  link_index_col,\n",
        "                  rating_threshold=3):\n",
        "    \"\"\"Loads csv containing edges between users and items\n",
        "\n",
        "    Args:\n",
        "        src_index_col (str): column name of users\n",
        "        dst_index_col (str): column name of items\n",
        "        link_index_col (str): column name of user item interaction\n",
        "        rating_threshold (int, optional): Threshold to determine positivity of edge. Defaults to 4.\n",
        "\n",
        "    Returns:\n",
        "        list of list: edge_index -- 2 by N matrix containing the node ids of N user-item edges\n",
        "        N here is the number of interactions\n",
        "    \"\"\"\n",
        "\n",
        "    edge_index = None\n",
        "\n",
        "    # Constructing COO format edge_index from input rating events\n",
        "\n",
        "    # get user_ids from rating events in the order of occurance\n",
        "    src = [user_id for user_id in  df['user_id']]\n",
        "    # get movie_id from rating events in the order of occurance\n",
        "    dst = [(movie_id) for movie_id in df['movie_id']]\n",
        "\n",
        "    # apply rating threshold\n",
        "    edge_attr = torch.from_numpy(df[link_index_col].values).view(-1, 1).to(torch.long) >= rating_threshold\n",
        "\n",
        "    edge_index = [[], []]\n",
        "    for i in range(edge_attr.shape[0]):\n",
        "        if edge_attr[i]:\n",
        "            edge_index[0].append(src[i])\n",
        "            edge_index[1].append(dst[i])\n",
        "    return edge_index"
      ],
      "metadata": {
        "id": "kwMgc3SOjtut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index = load_edge_csv(\n",
        "    ratings,\n",
        "    src_index_col='user_id',\n",
        "    dst_index_col='movie_id',\n",
        "    link_index_col='rating',\n",
        "    # TOOD: this is where you need to change your rating threshold - based off of what is put here\n",
        "    rating_threshold=3.5,\n",
        ")\n",
        "\n",
        "print(f\"{len(edge_index)} x {len(edge_index[0])}\")"
      ],
      "metadata": {
        "id": "MyBEZ-9wjveY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index = torch.LongTensor(edge_index)\n",
        "print(edge_index)\n",
        "print(edge_index.size())\n",
        "\n",
        "# Note: this is the total num_users and num_movies before we apply the rating_threshold\n",
        "num_users = len(ratings['user_id'].unique())\n",
        "num_movies = len(ratings['movie_id'].unique())\n",
        "\n",
        "num_interactions = edge_index.shape[1]\n",
        "\n",
        "# split the edges of the graph using a 80/10/10 train/validation/test split\n",
        "all_indices = [i for i in range(num_interactions)]\n",
        "\n",
        "train_indices, test_indices = train_test_split(all_indices,\n",
        "                                               test_size=0.2,\n",
        "                                               random_state=1)\n",
        "\n",
        "val_indices, test_indices = train_test_split(test_indices,\n",
        "                                             test_size=0.5,\n",
        "                                             random_state=1)\n",
        "\n",
        "train_edge_index = edge_index[:, train_indices]\n",
        "val_edge_index = edge_index[:, val_indices]\n",
        "test_edge_index = edge_index[:, test_indices]\n",
        "\n",
        "\n",
        "print(f\"num_users {num_users}, num_movies {num_movies}, num_interactions {num_interactions}\")\n",
        "print(f\"train_edge_index {train_edge_index}\")\n",
        "print((num_users + num_movies))\n",
        "print(torch.unique(train_edge_index[0]).size())\n",
        "print(torch.unique(train_edge_index[1]).size())\n",
        "\n",
        "def convert_r_mat_edge_index_to_adj_mat_edge_index(input_edge_index):\n",
        "    R = torch.zeros((num_users, num_movies))\n",
        "    for i in range(len(input_edge_index[0])):\n",
        "        row_idx = input_edge_index[0][i]\n",
        "        col_idx = input_edge_index[1][i]\n",
        "        R[row_idx][col_idx] = 1\n",
        "\n",
        "    R_transpose = torch.transpose(R, 0, 1)\n",
        "    adj_mat = torch.zeros((num_users + num_movies , num_users + num_movies))\n",
        "    adj_mat[: num_users, num_users :] = R.clone()\n",
        "    adj_mat[num_users :, : num_users] = R_transpose.clone()\n",
        "    adj_mat_coo = adj_mat.to_sparse_coo()\n",
        "    adj_mat_coo = adj_mat_coo.indices()\n",
        "    return adj_mat_coo\n",
        "\n",
        "\n",
        "def convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index):\n",
        "    sparse_input_edge_index = SparseTensor(row=input_edge_index[0],\n",
        "                                           col=input_edge_index[1],\n",
        "                                           sparse_sizes=((num_users + num_movies), num_users + num_movies))\n",
        "    adj_mat = sparse_input_edge_index.to_dense()\n",
        "    interact_mat = adj_mat[: num_users, num_users :]\n",
        "    r_mat_edge_index = interact_mat.to_sparse_coo().indices()\n",
        "    return r_mat_edge_index\n",
        "\n",
        "\n",
        "# convert from r_mat (interaction matrix) edge index to adjescency matrix's edge index\n",
        "# so we can feed it to model\n",
        "train_edge_index = convert_r_mat_edge_index_to_adj_mat_edge_index(train_edge_index)\n",
        "val_edge_index = convert_r_mat_edge_index_to_adj_mat_edge_index(val_edge_index)\n",
        "test_edge_index = convert_r_mat_edge_index_to_adj_mat_edge_index(test_edge_index)\n",
        "\n",
        "\n",
        "print(train_edge_index)\n",
        "print(train_edge_index.size())\n",
        "print(val_edge_index)\n",
        "print(val_edge_index.size())\n",
        "print(test_edge_index)\n",
        "print(test_edge_index.size())"
      ],
      "metadata": {
        "id": "xd6WOqBEkNRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_mini_batch(batch_size, edge_index):\n",
        "    \"\"\"Randomly samples indices of a minibatch given an adjacency matrix\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): minibatch size\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "\n",
        "    Returns:\n",
        "        tuple: user indices, positive item indices, negative item indices\n",
        "    \"\"\"\n",
        "    # structured_negative_sampling is a pyG library\n",
        "    # Samples a negative edge :obj:`(i,k)` for every positive edge\n",
        "    # :obj:`(i,j)` in the graph given by :attr:`edge_index`, and returns it as a\n",
        "    # tuple of the form :obj:`(i,j,k)`.\n",
        "    #\n",
        "    #         >>> edge_index = torch.as_tensor([[0, 0, 1, 2],\n",
        "    #         ...                               [0, 1, 2, 3]])\n",
        "    #         >>> structured_negative_sampling(edge_index)\n",
        "    #         (tensor([0, 0, 1, 2]), tensor([0, 1, 2, 3]), tensor([2, 3, 0, 2]))\n",
        "    edges = structured_negative_sampling(edge_index)\n",
        "\n",
        "    # 3 x edge_index_len\n",
        "    edges = torch.stack(edges, dim=0)\n",
        "\n",
        "    # here is whhen we actually perform the batch sampe\n",
        "    # Return a k sized list of population elements chosen with replacement.\n",
        "    indices = random.choices([i for i in range(edges[0].shape[0])], k=batch_size)\n",
        "\n",
        "    batch = edges[:, indices]\n",
        "\n",
        "    user_indices, pos_item_indices, neg_item_indices = batch[0], batch[1], batch[2]\n",
        "    return user_indices, pos_item_indices, neg_item_indices"
      ],
      "metadata": {
        "id": "ftW2HKUOkm3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defines LightGCN model\n",
        "class LightGCN(MessagePassing):\n",
        "    \"\"\"LightGCN Model as proposed in https://arxiv.org/abs/2002.02126\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_users,\n",
        "                 num_items,\n",
        "                 embedding_dim=64, # define the embding vector length for each node\n",
        "                 K=3,\n",
        "                 add_self_loops=False):\n",
        "        \"\"\"Initializes LightGCN Model\n",
        "\n",
        "        Args:\n",
        "            num_users (int): Number of users\n",
        "            num_items (int): Number of items\n",
        "            embedding_dim (int, optional): Dimensionality of embeddings. Defaults to 8.\n",
        "            K (int, optional): Number of message passing layers. Defaults to 3.\n",
        "            add_self_loops (bool, optional): Whether to add self loops for message passing. Defaults to False.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.K = K\n",
        "        self.add_self_loops = add_self_loops\n",
        "\n",
        "        # define user and item embedding for direct look up.\n",
        "        # embedding dimension: num_user/num_item x embedding_dim\n",
        "\n",
        "        self.users_emb = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.embedding_dim) # e_u^0\n",
        "\n",
        "        self.items_emb = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.embedding_dim) # e_i^0\n",
        "\n",
        "        # \"Fills the input Tensor with values drawn from the normal distribution\"\n",
        "        # according to LightGCN paper, this gives better performance\n",
        "        nn.init.normal_(self.users_emb.weight, std=0.1)\n",
        "        nn.init.normal_(self.items_emb.weight, std=0.1)\n",
        "\n",
        "    def forward(self, edge_index: Tensor):\n",
        "\n",
        "        edge_index_norm = gcn_norm(edge_index=edge_index,\n",
        "                                   add_self_loops=self.add_self_loops)\n",
        "\n",
        "        # concat the user_emb and item_emb as the layer0 embing matrix\n",
        "        # size will be (n_users + n_items) x emb_vector_len.   e.g: 10334 x 64\n",
        "        emb_0 = torch.cat([self.users_emb.weight, self.items_emb.weight]) # E^0\n",
        "\n",
        "        embs = [emb_0] # save the layer0 emb to the embs list\n",
        "\n",
        "        # emb_k is the emb that we are actually going to push it through the graph layers\n",
        "        # as described in lightGCN paper formula 7\n",
        "        emb_k = emb_0\n",
        "\n",
        "        # push the embedding of all users and items through the Graph Model K times.\n",
        "        # K here is the number of layers\n",
        "        for i in range(self.K):\n",
        "            emb_k = self.propagate(edge_index=edge_index_norm[0], x=emb_k, norm=edge_index_norm[1])\n",
        "            embs.append(emb_k)\n",
        "\n",
        "\n",
        "        # this is doing the formula8 in LightGCN paper\n",
        "\n",
        "        # the stacked embs is a list of embedding matrix at each layer\n",
        "        #    it's of shape n_nodes x (n_layers + 1) x emb_vector_len.\n",
        "        #        e.g: torch.Size([10334, 4, 64])\n",
        "        embs = torch.stack(embs, dim=1)\n",
        "\n",
        "        # From LightGCn paper: \"In our experiments, we find that setting α_k uniformly as 1/(K + 1)\n",
        "        #    leads to good performance in general.\"\n",
        "        emb_final = torch.mean(embs, dim=1) # E^K\n",
        "\n",
        "\n",
        "        # splits into e_u^K and e_i^K\n",
        "        users_emb_final, items_emb_final = torch.split(emb_final, [self.num_users, self.num_items])\n",
        "\n",
        "        # returns e_u^K, e_u^0, e_i^K, e_i^0\n",
        "        # here using .weight to get the tensor weights from n.Embedding\n",
        "        return users_emb_final, self.users_emb.weight, items_emb_final, self.items_emb.weight\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        # x_j is of shape:  edge_index_len x emb_vector_len\n",
        "        #    e.g: torch.Size([77728, 64]\n",
        "        #\n",
        "        # x_j is basically the embedding of all the neighbors based on the src_list in coo edge index\n",
        "        #\n",
        "        # elementwise multiply by the symmetrically norm. So it's essentiall what formula 7 in LightGCN\n",
        "        # paper does but here we are using edge_index rather than Adj Matrix\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "layers = 3\n",
        "model = LightGCN(num_users=num_users,\n",
        "                 num_items=num_movies,\n",
        "                 K=layers)"
      ],
      "metadata": {
        "id": "d64Ziz6OlKdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bpr_loss(users_emb_final,\n",
        "             users_emb_0,\n",
        "             pos_items_emb_final,\n",
        "             pos_items_emb_0,\n",
        "             neg_items_emb_final,\n",
        "             neg_items_emb_0,\n",
        "             lambda_val):\n",
        "    \"\"\"Bayesian Personalized Ranking Loss as described in https://arxiv.org/abs/1205.2618\n",
        "\n",
        "    Args:\n",
        "        users_emb_final (torch.Tensor): e_u_k\n",
        "        users_emb_0 (torch.Tensor): e_u_0\n",
        "        pos_items_emb_final (torch.Tensor): positive e_i_k\n",
        "        pos_items_emb_0 (torch.Tensor): positive e_i_0\n",
        "        neg_items_emb_final (torch.Tensor): negative e_i_k\n",
        "        neg_items_emb_0 (torch.Tensor): negative e_i_0\n",
        "        lambda_val (float): lambda value for regularization loss term\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: scalar bpr loss value\n",
        "    \"\"\"\n",
        "    reg_loss = lambda_val * (users_emb_0.norm(2).pow(2) +\n",
        "                             pos_items_emb_0.norm(2).pow(2) +\n",
        "                             neg_items_emb_0.norm(2).pow(2)) # L2 loss\n",
        "\n",
        "    pos_scores = torch.mul(users_emb_final, pos_items_emb_final)\n",
        "    pos_scores = torch.sum(pos_scores, dim=-1) # predicted scores of positive samples\n",
        "    neg_scores = torch.mul(users_emb_final, neg_items_emb_final)\n",
        "    neg_scores = torch.sum(neg_scores, dim=-1) # predicted scores of negative samples\n",
        "\n",
        "\n",
        "    bpr_loss = -torch.mean(torch.nn.functional.softplus(pos_scores - neg_scores))\n",
        "\n",
        "    loss = bpr_loss + reg_loss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "C3mX2e0WlfOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_positive_items(edge_index):\n",
        "    \"\"\"\n",
        "    Generates dictionary of positive items for each user\n",
        "\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "\n",
        "    Returns:\n",
        "        dict: user -> list of positive items for each\n",
        "    \"\"\"\n",
        "\n",
        "    # key: user_id, val: item_id list\n",
        "    user_pos_items = {}\n",
        "\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        user = edge_index[0][i].item()\n",
        "        item = edge_index[1][i].item()\n",
        "\n",
        "        if user not in user_pos_items:\n",
        "            user_pos_items[user] = []\n",
        "\n",
        "        user_pos_items[user].append(item)\n",
        "\n",
        "    return user_pos_items"
      ],
      "metadata": {
        "id": "hC1SyP94ljHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes recall@K and precision@K\n",
        "def RecallPrecision_ATk(groundTruth, r, k):\n",
        "    \"\"\"Computers recall @ k and precision @ k\n",
        "\n",
        "    Args:\n",
        "        groundTruth (list[list[long]]): list of lists of item_ids. Cntaining highly rated items of each user.\n",
        "                            In other words, this is the list of true_relevant_items for each user\n",
        "\n",
        "        r (list[list[boolean]]): list of lists indicating whether each top k item recommended to each user\n",
        "                            is a top k ground truth (true relevant) item or not\n",
        "\n",
        "        k (int): determines the top k items to compute precision and recall on\n",
        "\n",
        "    Returns:\n",
        "        tuple: recall @ k, precision @ k\n",
        "    \"\"\"\n",
        "\n",
        "    # number of correctly predicted items per user\n",
        "    # -1 here means I want to sum at the inner most dimension\n",
        "    num_correct_pred = torch.sum(r, dim=-1)\n",
        "\n",
        "    # number of items liked by each user in the test set\n",
        "    user_num_liked = torch.Tensor([len(groundTruth[i]) for i in range(len(groundTruth))])\n",
        "\n",
        "    recall = torch.mean(num_correct_pred / user_num_liked)\n",
        "    precision = torch.mean(num_correct_pred) / k\n",
        "    return recall.item(), precision.item()"
      ],
      "metadata": {
        "id": "wJXycLQWlkxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes NDCG@K\n",
        "def NDCGatK_r(groundTruth, r, k):\n",
        "    \"\"\"Computes Normalized Discounted Cumulative Gain (NDCG) @ k\n",
        "\n",
        "    Args:\n",
        "        groundTruth (list): list of lists containing highly rated items of each user\n",
        "        r (list): list of lists indicating whether each top k item recommended to each user\n",
        "            is a top k ground truth item or not\n",
        "        k (int): determines the top k items to compute ndcg on\n",
        "\n",
        "    Returns:\n",
        "        float: ndcg @ k\n",
        "    \"\"\"\n",
        "    assert len(r) == len(groundTruth)\n",
        "\n",
        "    test_matrix = torch.zeros((len(r), k))\n",
        "\n",
        "    for i, items in enumerate(groundTruth):\n",
        "        length = min(len(items), k)\n",
        "        test_matrix[i, :length] = 1\n",
        "    max_r = test_matrix\n",
        "    idcg = torch.sum(max_r * 1. / torch.log2(torch.arange(2, k + 2)), axis=1)\n",
        "    dcg = r * (1. / torch.log2(torch.arange(2, k + 2)))\n",
        "    dcg = torch.sum(dcg, axis=1)\n",
        "    idcg[idcg == 0.] = 1.\n",
        "    ndcg = dcg / idcg\n",
        "    ndcg[torch.isnan(ndcg)] = 0.\n",
        "    return torch.mean(ndcg).item()"
      ],
      "metadata": {
        "id": "L-L7V3zplmOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper function to get evaluation metrics - this is the code you need to look at when writing the code to predict shiz\n",
        "def get_metrics(model,\n",
        "                input_edge_index, # adj_mat based edge index\n",
        "                input_exclude_edge_indices, # adj_mat based exclude edge index\n",
        "                k):\n",
        "    \"\"\"Computes the evaluation metrics: recall, precision, and ndcg @ k\n",
        "\n",
        "    Args:\n",
        "        model (LighGCN): lightgcn model\n",
        "\n",
        "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
        "\n",
        "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "\n",
        "        k (int): determines the top k items to compute metrics on\n",
        "\n",
        "    Returns:\n",
        "        tuple: recall @ k, precision @ k, ndcg @ k\n",
        "    \"\"\"\n",
        "    # get the embedding tensor at layer 0 after training\n",
        "    user_embedding = model.users_emb.weight\n",
        "    item_embedding = model.items_emb.weight\n",
        "\n",
        "\n",
        "    # convert adj_mat based edge index to r_mat based edge index so we have have\n",
        "    # the first list being user_ids and second list being item_ids for the edge index\n",
        "    edge_index = convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index)\n",
        "\n",
        "    # This is to exclude the edges we have seen before in our predicted interaction matrix (r_mat_rating)\n",
        "    # E.g: for validation set, we want want to exclude all the edges in training set\n",
        "    exclude_edge_indices = [convert_adj_mat_edge_index_to_r_mat_edge_index(exclude_edge_index) \\\n",
        "                                      for exclude_edge_index in input_exclude_edge_indices]\n",
        "\n",
        "\n",
        "\n",
        "    # Generate predicted interaction matrix (r_mat_rating)\n",
        "    # (num_users x 64) dot_product (num_item x 64).T\n",
        "    r_mat_rating = torch.matmul(user_embedding, item_embedding.T)\n",
        "\n",
        "    # shape: num_users x num_item\n",
        "    rating = r_mat_rating\n",
        "\n",
        "    for exclude_edge_index in exclude_edge_indices:\n",
        "        # gets all the positive items for each user from the edge index\n",
        "        # it's a dict: user -> positive item list\n",
        "        user_pos_items = get_user_positive_items(exclude_edge_index)\n",
        "\n",
        "        # get coordinates of all edges to exclude\n",
        "        exclude_users = []\n",
        "        exclude_items = []\n",
        "        for user, items in user_pos_items.items():\n",
        "            # [user] * len(item) can give us [user1, user1, user1...] with len of len(item)\n",
        "            # this makes easier to do the masking below\n",
        "            exclude_users.extend([user] * len(items))\n",
        "            exclude_items.extend(items)\n",
        "\n",
        "        # set the excluded entry in the rat_mat_rating matrix to a very small number\n",
        "        rating[exclude_users, exclude_items] = -(1 << 10)\n",
        "\n",
        "    # get the top k recommended items for each user\n",
        "    _, top_K_items = torch.topk(rating, k=k)\n",
        "\n",
        "    # get all unique users in evaluated split\n",
        "    users = edge_index[0].unique()\n",
        "\n",
        "    # dict of user -> pos_item list\n",
        "    test_user_pos_items = get_user_positive_items(edge_index)\n",
        "\n",
        "    # convert test user pos items dictionary into a list of lists\n",
        "    test_user_pos_items_list = [test_user_pos_items[user.item()] for user in users]\n",
        "\n",
        "\n",
        "    # r here is \"pred_relevant_items ∩ actually_relevant_items\" list for each user\n",
        "    r = []\n",
        "    for user in users:\n",
        "        user_true_relevant_item = test_user_pos_items[user.item()]\n",
        "        # list of Booleans to store whether or not a given item in the top_K_items for a given user\n",
        "        # is also present in user_true_relevant_item.\n",
        "        # this is later on used to compute n_rel_and_rec_k\n",
        "        label = list(map(lambda x: x in user_true_relevant_item, top_K_items[user]))\n",
        "        r.append(label)\n",
        "\n",
        "    r = torch.Tensor(np.array(r).astype('float'))\n",
        "\n",
        "    recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, k)\n",
        "    ndcg = NDCGatK_r(test_user_pos_items_list, r, k)\n",
        "\n",
        "    return recall, precision, ndcg"
      ],
      "metadata": {
        "id": "1B5FX820ln-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper function to evaluate model\n",
        "def evaluation(model,\n",
        "               edge_index, # adj_mat based edge index\n",
        "               exclude_edge_indices,  # adj_mat based exclude edge index\n",
        "               k,\n",
        "               lambda_val\n",
        "              ):\n",
        "    \"\"\"Evaluates model loss and metrics including recall, precision, ndcg @ k\n",
        "\n",
        "    Args:\n",
        "        model (LighGCN): lightgcn model\n",
        "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
        "        sparse_edge_index (sparseTensor): sparse adjacency matrix for split to evaluate\n",
        "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "        k (int): determines the top k items to compute metrics on\n",
        "        lambda_val (float): determines lambda for bpr loss\n",
        "\n",
        "    Returns:\n",
        "        tuple: bpr loss, recall @ k, precision @ k, ndcg @ k\n",
        "    \"\"\"\n",
        "    # get embeddings\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(edge_index)\n",
        "\n",
        "    r_mat_edge_index = convert_adj_mat_edge_index_to_r_mat_edge_index(edge_index)\n",
        "\n",
        "    edges = structured_negative_sampling(r_mat_edge_index, contains_neg_self_loops=False)\n",
        "\n",
        "    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n",
        "\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[pos_item_indices], items_emb_0[pos_item_indices]\n",
        "\n",
        "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "    loss = bpr_loss(users_emb_final,\n",
        "                    users_emb_0,\n",
        "                    pos_items_emb_final,\n",
        "                    pos_items_emb_0,\n",
        "                    neg_items_emb_final,\n",
        "                    neg_items_emb_0,\n",
        "                    lambda_val).item()\n",
        "\n",
        "\n",
        "    recall, precision, ndcg = get_metrics(model,\n",
        "                                          edge_index,\n",
        "                                          exclude_edge_indices,\n",
        "                                          k)\n",
        "\n",
        "    return loss, recall, precision, ndcg"
      ],
      "metadata": {
        "id": "w0ruZA7ilprv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define contants\n",
        "ITERATIONS = 10000\n",
        "EPOCHS = 10\n",
        "# ITERATIONS = 500\n",
        "BATCH_SIZE = 1024\n",
        "LR = 1e-3\n",
        "ITERS_PER_EVAL = 200\n",
        "ITERS_PER_LR_DECAY = 200\n",
        "K = 20\n",
        "LAMBDA = 1e-6\n",
        "# LAMBDA = 1/2"
      ],
      "metadata": {
        "id": "y-oyNVcXlrF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device {device}.\")\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "edge_index = edge_index.to(device)\n",
        "train_edge_index = train_edge_index.to(device)\n",
        "val_edge_index = val_edge_index.to(device)"
      ],
      "metadata": {
        "id": "eCN1KKN6ltNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embs_for_bpr(model, input_edge_index):\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(input_edge_index)\n",
        "\n",
        "\n",
        "    edge_index_to_use = convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index)\n",
        "\n",
        "    # mini batching for eval and calculate loss\n",
        "    user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(BATCH_SIZE, edge_index_to_use)\n",
        "\n",
        "    # This is to push tensor to device so if we are using GPU\n",
        "    user_indices, pos_item_indices, neg_item_indices = user_indices.to(device), pos_item_indices.to(device), neg_item_indices.to(device)\n",
        "\n",
        "\n",
        "    # we need layer0 embeddings and the final embeddings (computed from 0...K layer) for BPR loss computing\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[pos_item_indices], items_emb_0[pos_item_indices]\n",
        "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "    return users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0\n",
        "\n",
        "# training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_recall_at_ks = []\n",
        "\n",
        "for iter in tqdm(range(ITERATIONS)):\n",
        "    # forward propagation\n",
        "    users_emb_final, users_emb_0,  pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0 \\\n",
        "                = get_embs_for_bpr(model, train_edge_index)\n",
        "\n",
        "    # loss computation\n",
        "    train_loss = bpr_loss(users_emb_final,\n",
        "                          users_emb_0,\n",
        "                          pos_items_emb_final,\n",
        "                          pos_items_emb_0,\n",
        "                          neg_items_emb_final,\n",
        "                          neg_items_emb_0,\n",
        "                          LAMBDA)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # validation set\n",
        "    if iter % ITERS_PER_EVAL == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_loss, recall, precision, ndcg = evaluation(model,\n",
        "                                                           val_edge_index,\n",
        "                                                           [train_edge_index],\n",
        "                                                           K,\n",
        "                                                           LAMBDA\n",
        "                                                          )\n",
        "\n",
        "            print(f\"[Iteration {iter}/{ITERATIONS}] train_loss: {round(train_loss.item(), 5)}, val_loss: {round(val_loss, 5)}, val_recall@{K}: {round(recall, 5)}, val_precision@{K}: {round(precision, 5)}, val_ndcg@{K}: {round(ndcg, 5)}\")\n",
        "\n",
        "            train_losses.append(train_loss.item())\n",
        "            val_losses.append(val_loss)\n",
        "            val_recall_at_ks.append(round(recall, 5))\n",
        "        model.train()\n",
        "\n",
        "    if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n",
        "        scheduler.step()\n",
        "\n",
        "iters = [iter * ITERS_PER_EVAL for iter in range(len(train_losses))]\n",
        "plt.plot(iters, train_losses, label='train')\n",
        "plt.plot(iters, val_losses, label='validation')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.title('training and validation loss curves')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nc7Ey2HolwZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "knHrMAR1oU_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiZ12Z2JegPY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kzN0FAvegPY"
      },
      "outputs": [],
      "source": [
        "# TODO: - visualisations of the data, pick out features of the dataset that validate your choices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osapThstegPZ"
      },
      "outputs": [],
      "source": [
        "# load the data into the suprise module\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "data = Dataset.load_from_df(ratings[[\"user_id\", \"movie_id\", \"rating\"]], reader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rosjMCIegPZ"
      },
      "outputs": [],
      "source": [
        "# run algorithm\n",
        "kf = KFold(n_splits=3)\n",
        "\n",
        "basic_algo = SVD()\n",
        "\n",
        "for trainset, testset in kf.split(data):\n",
        "\n",
        "    # train and test algorithm.\n",
        "    basic_algo.fit(trainset)\n",
        "    predictions = basic_algo.test(testset)\n",
        "\n",
        "    # Compute and print Root Mean Squared Error\n",
        "    accuracy.rmse(predictions, verbose=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UE6XqrhegPZ"
      },
      "outputs": [],
      "source": [
        "print(\"Leave the input box empty to exit the program\")\n",
        "while True:\n",
        "    user_id = input(\"Please enter a user_id to predict movie recommendations for: \")\n",
        "    if user_id == \"\":\n",
        "        break\n",
        "    user_exists = users['user_id'].isin([user_id]).any()\n",
        "    if not user_exists:\n",
        "        print(\"User not found\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Calculating top 5 movies that 'user {user_id}' will like...\")\n",
        "\n",
        "    # Retrieve the age of the user\n",
        "    user_age = users.loc[users['user_id'] == user_id, 'age'].iloc[0]\n",
        "\n",
        "    movie_ids = movies['movie_id'].unique()\n",
        "    predictions = []\n",
        "    for movie_id in movie_ids:\n",
        "        prediction = basic_algo.predict(user_id, str(movie_id))\n",
        "        predictions.append((movie_id, prediction.est))\n",
        "\n",
        "    # Sort by estimated rating\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    top_n = 5\n",
        "    top_predictions = []\n",
        "    for movie_id, rating in predictions:\n",
        "        movie_age_rating = movies.loc[movies['movie_id'] == movie_id, 'age_rating'].iloc[0]\n",
        "\n",
        "        # Check if the age rating is NA or appropriate for the user's age\n",
        "        if pd.isna(movie_age_rating) or movie_age_rating <= user_age:\n",
        "            top_predictions.append((movie_id, rating))\n",
        "            if len(top_predictions) == top_n:\n",
        "                break\n",
        "\n",
        "    print(\"\")\n",
        "    for i, prediction in enumerate(top_predictions):\n",
        "        movie_id = prediction[0]\n",
        "        movie_name = movies.loc[movies['movie_id'] == movie_id, 'title'].iloc[0]\n",
        "        print(f\"{i+1}. {movie_name}\")\n",
        "        print(\"\")\n",
        "\n",
        "print(\"Program exited\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}